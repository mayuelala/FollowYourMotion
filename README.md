<div align="center">
<h2><font color="red"> Follow-Your-Motion </font></center> <br> <center>Video Motion Transfer via Efficient Spatial-Temporal Decoupled Finetuning</h2>


[Yue Ma*](https://mayuelala.github.io/), [Yulong Liu*](https://scholar.google.com.my/citations?user=GZC_7c4AAAAJ), [Qiyuan Zhu*](https://github.com/follow-your-motion), [Ayden Yang](https://github.com/follow-your-motion), [Kunyu Feng](https://github.com/fkyyyy), [Xinhua Zhang](https://github.com/NXZXH), [Zhifeng Li](https://scholar.google.com/citations?user=VTrRNN4AAAAJ&hl=zh-CN),  
[Sirui Han](https://facultyprofiles.hkust.edu.hk/profiles.php?profile=sirui-han-siruihan), [Chenyang Qi](https://chenyangqiqi.github.io/) and [Qifeng Chen](https://scholar.google.com/citations?user=lLMX9hcAAAAJ&hl=en)

<a href='https://arxiv.org/abs/2506.05207'><img src='https://img.shields.io/badge/ArXiv-2403.08268-red'></a> 
<a href='https://follow-your-motion.github.io/'><img src='https://img.shields.io/badge/Project-Page-Green'></a>  
![visitors](https://visitor-badge.laobi.icu/badge?page_id=mayuelala.FollowYourMotion&left_color=green&right_color=red)  [![GitHub](https://img.shields.io/github/stars/mayuelala/FollowYourMotion?style=social)](https://github.com/mayuelala/FollowYourMotion) 
</div>




# ğŸ–¼ Gallery

We have showcased some results of Follow-Your-Motion .

More results can be found on our [Project page](https://follow-your-motion.github.io/).


<table>
<td><img src="assets\1.gif">

<img src="assets\3.gif">
<img src="assets\5.gif">
<img src="assets\7.gif">
<img src="assets\9.gif">
<img src="assets\11.gif">
<img src="assets\13.gif">
<img src="assets\15.gif">
<img src="assets\17.gif">
<img src="assets\19.gif">
<img src="assets\21.gif">
<img src="assets\23.gif">
<img src="assets\25.gif">
<img src="assets\27.gif">
<img src="assets\29.gif"></td>
</table>


# ğŸ“ Note  
### ğŸ•¹ We are cleaning the code and creating a demo. We really want everybody to try it! 
### ğŸ˜Š The code and checkpoints is coming soonï¼
### ğŸ’— Thanks for your attention! If you are interested in our work, please give us a star â­ï¸â­ï¸â­ to let us know.
### ğŸš€ We will speed up the development! 


# ğŸ‘¨â€ğŸ‘©â€ğŸ‘§â€ğŸ‘¦ Follow Family
[Follow-Your-Pose](https://github.com/mayuelala/FollowYourPose): Pose-Guided text-to-Video Generation.

[Follow-Your-Click](https://github.com/mayuelala/FollowYourClick): Open-domain Regional image animation via Short Prompts.
  

## â­ï¸ Star History

[![Star History Chart](https://api.star-history.com/svg?repos=mayuelala/FollowYourMotion&type=Date)](https://star-history.com/#mayuelala/FollowYourMotion&Date)



# ğŸ¼ Citation 
If you think this project is helpful, please feel free to leave a starâ­ï¸â­ï¸â­ï¸ and cite our paper:
```bibtex
@article{ma2025follow,
  title={Follow-Your-Motion: Video Motion Transfer via Efficient Spatial-Temporal Decoupled Finetuning},
  author={Ma, Yue and Liu, Yulong and Zhu, Qiyuan and Yang, Ayden and Feng, Kunyu and Zhang, Xinhua and Li, Zhifeng and Han, Sirui and Qi, Chenyang and Chen, Qifeng},
  journal={arXiv preprint arXiv:2506.05207},
  year={2025}
}
``` 

